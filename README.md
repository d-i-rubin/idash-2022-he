# idash-2022-he

IDASH 2022 Solution 1 Report L-Infinity Labs
Daniel Rubin, CTO
The challenge we have undertaken is to develop models that can infer each of 5 patient phenotype values (3 continuous, 2 binary) from a patient genotype variant feature vector of size 23,090, each entry either 0, 1, or 2 corresponding to that many copies of an uncommon variant at the corresponding position. These trained models must be evaluated on a test set of 198 samples in a secure, privacy-preserving way, meaning the genotype data must be encrypted in a homomorphic fashion, as will be the model data, both of which are sent to an independent server for evaluation. The owner of the data can then decrypt the results of the model evaluations.
Detailed nature of input and output: Our solution requires an input of test genome samples from the user. This file is assumed to be ‘test_genotypes.txt.gz’ in the same form as the original compressed challenge training data, containing 198 samples instead of 3000, and placed within the folder inside the package called ‘input’. That is, when opened the file should be an array of 23,090 rows and 198+4 columns, the first 4 of which will be ignored. This input folder comes with a subfolder called ‘models’ which contains the model data.
The output of our solution is a csv file called ‘predictions.csv’ of size (198,5) (with no header or row labels). Each row of this array contains the predictions of the phenotype values for Phenotypes 1 through 3 in slots 0 to 2, and slots 3 and 4 contain the decision functions for Phenotypes 4 and 5. To calculate the ROC score for phenotype 4, for example, one could use the method from sklearn.metrics roc_auc_score, and the command “roc_auc_score( ph4, predictions[:,3] )”, where ph4 is a list or array of 1’s and 0’s representing the correct binary classifications of the test samples. To convert decision function results into assigned categories for Phenotypes 4 and 5, the category is 1 if the decision function value is greater than or equal to 0, and 0 otherwise. We also generate a csv file ‘binary_predictions.csv’ of size (198,2) containing these class predictions for Phenotypes 4 and 5. These are placed in the package in a folder called ‘output’.
All ciphertext data and encryption context info that is generated and sent between the processes is placed in a folder called ‘scratch’.
How to run:
1. Download the compressed file using the link and decompress.
2. Place the file ‘test_genomes.txt.gz’ of precisely the same format as the original
compressed training data file but containing 198 samples into the folder marked ‘input’.
3. In the terminal, from the ‘package’ folder, enter the command
make load_docker
4. After loading the image, you can run the commands
make run_step1
 make run_step2 make run_step3 make run_step4
(which must be done in order), or run make run_all_steps
to automatically run all processes in order.
5. Retrieve the predictions as described above from the ‘output’ folder.
Model and Encryption Details: The primary difficulty in the data science aspect of the problem is that the number of features in the data is much greater than the number of samples. In this scenario, even simple linear models can perform extremely well when optimized on a training set, but due to overtraining perform terribly on new inputs. Our approach was to restrict linear models further by imposing an L1 penalty on the weights to force the models to be sparse. These are called Lasso models in the case of linear regression, which was used for the 3 continuous phenotype targets, and the same technique was used to train the coefficients of logistic regression models for the 2 binary phenotypes. Optimizing the strength of the L1 penalty for test accuracy, we found models that depended on between 10-200 features out of the 23,090, plus intercepts. Note that even with such simple models, we were able to achieve Normalized Root Mean Square Errors of about .03-.04, and AUC’s of .98-.99. Our models have 13, 14, 126, 62, and 136 nonzero weights, respectively for the 5 phenotypes. The feature numbers and nonzero weights of each model are in the ‘models’ folder.
Evaluation of these models is in principle a simple operation of taking a dot product between the feature weight vector (with an appended feature set equal to 1 for the intercept) and the model weight vector. Since this evaluation is only of depth 1 in terms of multiplications, our Leveled HE scheme requires only relatively small parameters. We encrypt using the CKKS scheme as implemented in the SEAL library with degree N=4096 and ciphertext modulus q of 100 bits (a product of 3 primes of size 35, 30, 35). This allows for ciphertexts with 2048 slots for floating-point entries. Hence each genotype sample is split into 12 vectors of size 2048 (the last appended with 1.0 for the intercept terms and then padded with 0’s) before encryption.
In Step 1, the client first loads the test genotype data. The only preprocessing we do is to take the transpose of the genotype dataframe and discard the first 4 rows that do not contain variant information. The client generates the encryption context, encrypts the genotype data, and saves the encrypted data and the context information. Note that in a real-world setting, we imagine that when using a leveled FHE scheme for outsourced inference, the model owner would instruct the client as to the required encryption context information, since the parameters are selected to make the evaluation algorithm work. The client generates the public key, galois keys, relinearization keys, and secret key. The model owner only needs the context information and the public key for Step 2; the public key, galois keys, and relinearlization keys and context info are used by the Evaluator in Step 3; and the secret key is used only by the client in Step 4.

 Our method of encoding and encrypting the model for evaluation is designed to minimize the number of rotation and addition operations that must be performed, at the expense of more multiplications undertaken in parallel. Since the models are sparse, we separately encode each nonzero term of each model as a vector of all zeros except for the weight/coefficient in the correct slot out of 2048. These lists of vectors are encrypted before sending to the server. In this method of evaluation, the model owner must send to the server in plaintext the index of the nonzero slot. This is because we evaluate by performing one termwise multiply between the each of the model weight vectors and the corresponding batch of 2048 data features, and then rotating the result by the appropriate number of slots so as to place the nonzero entry of the result in the first slot if the model vector is for predicting phenotype 1, 2 for phenotype 2, and so on up to 5. Once this is done for each nonzero weight for each of the 5 models, the result is summed. Upon decryption, this results in a vector of size 2048 for each test data sample whose first 5 entries are the model predictions for phenotypes 1-5 for that sample, and with all other entries containing noise generally of several orders of magnitude smaller than the nonzero entries. Actually, to be more precise, in order to ensure that the results decrypt to at least 3 places of accuracy and are easily distinguished from the noise of approximation in CKKS, the model weights are multiplied by a scale factor of 10^3 prior to encryption (which places them at about the same unit scale as the data entries), and the final results after decryption must be divided by the same 10^3. In addition, both 10^3 times the model weights and the data vectors are encoded in CKKS as plaintexts with a scale factor of 2^30, and we rescale by dividing out by the middle prime after the multiplication operation. We also relinearize the multiplied ciphertexts to bring them from size 3 to size 2 before rotating and adding. The time of the encrypted evaluation step varied on our home machines, but averaged about 1 second per sample.
We chose this method of performing a multiplication for each nonzero weight of each model over the naive approach to computing dot products in CKKS. The naive approach would be to do a single termwise multiply between the coefficient vector of a model with the data vector, and then sum all of the rotations in order to place the resulting dot product in every slot. This approach proved too costly in time and noise due to the number of rotations and additions. We were interested in a different approach whereby the data entries could be encoded directly as the coefficients of a plaintext polynomial (of half the degree of the degree of the ring) and the model coefficients encoded similarly but in the opposite order. Thus when multiplied and decrypted, the dot product can be extracted from the middle coefficient of the result. Unfortunately our team did not have time to implement this approach.
Let us comment on the privacy of this protocol. Although it is our understanding that this protocol satisfies the requirements of the IDASH challenge, it has several drawbacks from the perspective of a real use-case. For the data owner, there is no problem; the data is encrypted with >128-bit security and is completely hidden from the server and model owner. But much information about the model leaks. To the server, encryption of the weight vectors protects the actual weights from being seen by the server, but the need to identify the batch of 2048 and the number of rotations completely exposes the feature selection. Note that the encryption of the model could only ever serve to protect the model from the server owner and not the data owner. Since the data owner has black-box access to the model and the models are linear, the data

owner need only query with each unit vector in turn to learn all the weights of the models. Our view is that purely linear models can never really be hidden from a party with black-box access. This renders somewhat artificial the requirement of no preprocessing of the data that would reveal the feature selection.
In the case of the two binary phenotypes, the decision to report the linear result, or decision function number of the model, was done for ease of computation and also because one needs this number or the probability score (from which the decision function and hence the model weights could be determined) to evaluate the AUROC accuracy metric. If it were acceptable to report only the class that the model predicts to the data owner, then the model weights could be hidden from the data owner in the sense that for the data owner to recover the model the task is just as hard and requires about the same amount of training data to train a model that performs as well. We believe that the best real use cases for Homomorphic Encryption for model inference involve both private data and proprietary models (otherwise the model owner may as well make the model available in plaintext to clients). Thus in our view the real challenge in the case of classification models is to find a way to evaluate the model under HE in such a way as to report the assigned category with as little additional information as possible. Note that the L-Infinity team has accomplished this task in our unsubmitted solution to the IDASH 2021 Track 2 challenge.
